{
  "summary": "There is a problem with using maximum likelihood estimates for probabilities. Any finite training corpus will be missing some perfectly acceptable English word sequences. Discounting algorithms take-off a bit of probability mass from some more frequent events and give it to unseen events. Laplace smoothing merely adds one to each count (hence its alternate name addaddone one smoothing) Since there are V words in the vocabulary and each one was incremented, we also need to adjust the denominator to take into account the extra V observations. Smoothing, Backoff and Entropy CS702 B.Tech & M.Tech 2025 , Shreya Agarwal.\nThere are two ways to use this n-gram “hierarchy”. In backoff, we use the trigram if the evidence is sufficient. In interpolation, we always mix the probability estimates from all the n- gram estimators. We do so by choosing the ‘L’ values that maximize the likelihood of the held-out corpus. ncremented, we also need to adjust the denominator to take into account the extra V observations. Continued…. Question What happens to our P values if we don’t increase the numerator?",
  "refined": "Title: Understanding Smoothing, Backoff, and Interpolation in N-gram Models for Natural Language Processing (NLP)\n\nIn the field of Natural Language Processing (NLP), n-gram models are used to predict the probability of a sequence of words (also known as a 'word sequence'). However, these models can encounter challenges when using Maximum Likelihood Estimates (MLE) due to their inability to account for all possible English word sequences (a finite training corpus will inevitably miss some acceptable sequences).\n\nTo address this issue, we employ two techniques: smoothing and discounting. Discounting algorithms slightly reduce the probability mass from more frequent events and redistribute it to less common or unseen events. Laplace Smoothing is a popular example, where one is added to each count (hence its alternative name \"add-one smoothing\"). This is done because there are V words in the vocabulary, and incrementing each by one requires adjusting the denominator to account for the extra V observations.\n\nAnother technique is called the n-gram hierarchy. Here, we have two main approaches: backoff and interpolation. In Backoff, if the evidence is sufficient (e.g., we have enough data for a trigram), we use it; otherwise, we fallback to bigrams or unigrams. Interpolation, on the other hand, always mixes the probability estimates from all the n-gram estimators by selecting the 'L' values that maximize the likelihood of the held-out corpus (a set of data separate from the training data used for estimation).\n\nIt is essential to remember that not increasing the numerator when smoothing can lead to incorrect probability estimates, as it underestimates the frequency of certain word sequences. To illustrate this, consider a simple example where we have two words A and B, with counts 5 (A) and 2 (B). If we don't adjust for smoothing, our estimated probabilities would be P(A)=5/7 and P(B)=2/7. However, if we apply Laplace Smoothing by adding one to each count, the estimated probabilities become more balanced: P(A)≈6/8 and P(B)≈2/8. This is because Laplace smoothing adds a small amount of probability mass to less common events like B, making our estimates more accurate.\n\nKey Definitions:\n- Maximum Likelihood Estimates (MLE): An estimator of a parameter in statistics that optimizes the likelihood of an observed sample. In the context of n-gram models, it is used to estimate the probability of a given word sequence.\n- N-gram Model: A statistical model used in Natural Language Processing to predict the probability of a sequence of n words (n-gram).\n- Laplace Smoothing (add-one smoothing): A technique used to improve the estimation of probabilities by adding one to each count, ensuring that every event has some non-zero probability.\n- Backoff: An approach in n-gram hierarchy where we use a higher order n-gram if the evidence is sufficient; otherwise, we fallback to lower order n-grams (e.g., using bigram if trigram evidence is insufficient).\n- Interpolation: An approach in n-gram hierarchy where we always mix probability estimates from all n-gram estimators, with the values chosen based on maximizing the likelihood of a held-out corpus."
}
# EZLearn â€“ Summarization + RAG Pipeline ğŸš€

EZLearn is an end-to-end **Retrieval-Augmented Generation (RAG)** and **Summarization pipeline** designed to process PDFs and PPTX files, extract knowledge efficiently, and generate highâ€‘quality summaries or answers to user queries. It automates text extraction, chunking, vector embeddings, summarization, and interactive Q&A using LLMs.

---

## ğŸ” Features

- ğŸ“„ Extracts text from **PDF** and **PPTX** files  
- âœ¨ Cleans and preprocesses raw text  
- âœ‚ï¸ Chunks content for optimal retrieval  
- ğŸ§  Creates and stores **local vector embeddings**  
- ğŸ—‚ï¸ Saves vectorstores for fast reuse  
- ğŸ“ Generates summaries using a local or remote LLM  
- ğŸ“ Creates **educational summaries** using Ollama refinement  
- ğŸ” Allows interactive Q&A through a RAG loop  
- âš¡ Caches results to avoid reprocessing  

---

## ğŸ§± Pipeline Architecture

1. **Extraction** â€“ Convert PDF/PPTX files into raw text  
2. **Preprocessing** â€“ Clean, normalize, and chunk the text  
3. **Embedding** â€“ Generate embeddings using SentenceTransformers  
4. **Vectorstore** â€“ Save/reload embeddings for efficient retrieval  
5. **Summarization** â€“ Generate a concise summary of the entire document  
6. **Ollama Refinement** â€“ Produce an educational-style summary  
7. **RAG Q&A** â€“ Retrieve relevant chunks + answer user questions  
8. **Interactive Loop** â€“ Chat with your document indefinitely  

---

## ğŸ“ Project Structure

```
nlp_pipeline/
â”‚â”€â”€ main.py
â”‚â”€â”€ config.py
â”‚â”€â”€ project_pipeline/
â”‚     â”œâ”€â”€ extractors.py
â”‚     â”œâ”€â”€ preprocess.py
â”‚     â”œâ”€â”€ embeddings.py
â”‚     â”œâ”€â”€ summarizer.py
â”‚     â”œâ”€â”€ rag.py
```

---

## â–¶ï¸ How the Pipeline Works

1. User enters file path  
2. Pipeline checks if a cached vectorstore exists  
3. If yes â†’ loads vectorstore + summary  
4. If no â†’ processes file, builds embeddings, creates summaries  
5. Loads the LLM once  
6. Enters an infinite questionâ€‘answer loop using RAG

---

## ğŸ› ï¸ Tech Stack

- Python  
- PyPDF / python-pptx  
- SentenceTransformers  
- Ollama (Mistral model)  
- Local vector store (FAISS-like)  
- Torch  
- JSON for caching summaries  

---

## ğŸ”§ Configuration (config.py)

- **DEVICE** â€“ CPU/GPU auto-detection  
- **EMBEDDING_MODEL** â€“ MiniLM-L6-v2 (fast + accurate)  
- **LLM_MODEL** â€“ Local Mistral via Ollama  
- **CHUNK_SIZE** â€“ Characters per chunk  
- **CHUNK_OVERLAP** â€“ Overlap between chunks  
- **TOP_K** â€“ Retrieval depth  

---

## ğŸš€ Running the Pipeline

```
python main.py
Enter PDF or PPTX path: myfile.pdf
```

Then ask:

```
Ask a question (or type 'exit' to quit):
```

Example:

```
What is the main idea of chapter 2?
```

---

## ğŸ§  Example Output

### **Summary**
A concise explanation of the entire document.

### **Educational Summary**
A simplified explanation refined using a secondary LLM (Ollama Mistral).

### **RAG Answer**
Retrieves 3 most relevant chunks â†’ produces contextual answer.

---

## ğŸ¤ Contributing

Contributions, issues, and feature requests are welcome!  
Feel free to open a PR or create an issue.

---

## ğŸ“œ License

MIT License  

---

Enjoy using **EZLearn** to turn heavy documents into clean summaries and interactive knowledge! ğŸš€
